\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can reference the SRS tables if needed}

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

This document ...

\section{Functional Requirements Evaluation}

\section{Nonfunctional Requirements Evaluation}

\subsection{Usability}
		
\subsection{Performance}

\subsection{etc.}
	
\section{Comparison to Existing Implementation}	

\subsection{Overview of Existing Solutions}

\subsubsection{MOSS (Measure of Software Similarity)}
MOSS is currently the standard tool for detecting code plagiarism in academic settings. Developed at Stanford University, it works by:
\begin{itemize}
    \item Tokenizing code into a sequence of symbols
    \item Using document fingerprinting to detect similar code segments
    \item Generating a similarity score based on matching sequences
    \item Providing a web interface to view overlapping code segments
\end{itemize}

\subsubsection{Other Notable Solutions}
\begin{itemize}
    \item \textbf{JPlag}: Uses a token-based approach similar to MOSS, but with different tokenization strategies
    \item \textbf{PLAGGIE}: Java-specific plagiarism detection tool used in academic environments
    \item \textbf{CodeMatch}: Commercial solution that uses both tokenization and metric-based methods
\end{itemize}

\subsection{Our Implementation vs. Existing Solutions}

\subsubsection{Current Approach}
Our implementation leverages CodeBERT, a pre-trained model for programming language understanding, to perform code pair comparison. Key aspects include:
\begin{itemize}
    \item Utilization of transformer-based neural networks to understand code semantics
    \item Direct comparison of code pairs to determine similarity
    \item A modern, user-friendly interface for result visualization and analysis
\end{itemize}

\subsubsection{Functional Comparison}

\begin{table}[h]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Feature} & \textbf{MOSS} & \textbf{Our CodeBERT Implementation} \\
\hline
Detection Method & Token-based fingerprinting & Neural representation and semantic understanding \\
\hline
Language Support & Multiple languages & Multiple languages (supported by CodeBERT) \\
\hline
Semantic Understanding & Limited (syntax-focused) & Enhanced (captures semantic meaning) \\
\hline
Resistance to Obfuscation & Low-moderate & Potentially higher \\
\hline
Speed & Fast for large submissions & Currently slower for large-scale comparisons \\
\hline
Visualization & Basic web interface & Modern, interactive UI \\
\hline
\end{tabular}
\caption{Comparison between MOSS and our CodeBERT implementation}
\label{tab:comparison}
\end{table}

\subsection{Advantages of Our Approach}

\begin{enumerate}
    \item \textbf{Semantic Understanding}: Unlike MOSS's purely syntactic approach, CodeBERT can potentially understand the meaning behind code, making it more difficult to fool with non-functional additions.

    \item \textbf{Context Awareness}: Our implementation considers the context in which code appears, potentially reducing false positives from common solutions to standard problems.

    \item \textbf{Modern Interface}: Our user interface provides a more intuitive experience for instructors reviewing potential plagiarism cases.
\end{enumerate}

\subsection{Current Limitations and Trade-offs}

\begin{enumerate}
    \item \textbf{Computational Requirements}: Neural models like CodeBERT require more computational resources than traditional approaches like MOSS.

    \item \textbf{Development Maturity}: MOSS has been refined over decades, while our solution is still in the early stages of development.

    \item \textbf{Validation}: Our approach needs more extensive testing across different programming languages and assignments to prove its effectiveness.

    \item \textbf{Explainability}: Neural network decisions can be less transparent than token-matching approaches, making it potentially harder to explain why certain code pairs were flagged.
\end{enumerate}

\subsection{Design Decision Justification}

\begin{enumerate}
    \item \textbf{Choice of CodeBERT}: We selected CodeBERT over other models because it was specifically pre-trained on code from multiple programming languages, making it well-suited for understanding code semantics across different languages used in academic settings.

    \item \textbf{Focus on Pair Comparison}: While MOSS compares each submission against all others, our current approach focuses on direct pair comparison, allowing for more detailed analysis of potentially plagiarized code.

    \item \textbf{UI Priority}: We invested in a high-quality UI early in development to facilitate easier testing and validation of our detection results.
\end{enumerate}

\subsection{Future Improvements}

\begin{enumerate}
    \item \textbf{Hybrid Approach}: Combining neural understanding with traditional token-based methods to leverage strengths of both approaches.

    \item \textbf{Performance Optimization}: Improving the computational efficiency to handle large class submissions more effectively.

    \item \textbf{Tunable Sensitivity}: Implementing adjustable thresholds for different assignment types and programming languages.
\end{enumerate}

\section{Unit Testing}
\subsection{AnalyzeFiles Component}

\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{AnalyzeFiles} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Check for the presence of specific text.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the text "Analyze a Dataset" and "Upload a dataset to analyze for potential code plagiarism. Use a ZIP file containing your project files."
\end{itemize}

\subsubsection{Displays an alert when no file is uploaded}
\begin{itemize}
    \item \textbf{Description}: Ensures an alert is displayed when no file is uploaded.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Click the "Analyze Dataset" button.
    \end{enumerate}
    \item \textbf{Expected result}: An alert with the message "Please upload a dataset file first." is displayed.
\end{itemize}

\subsubsection{Displays an alert when no analysis name is entered}
\begin{itemize}
    \item \textbf{Description}: Ensures an alert is displayed when no analysis name is entered.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Upload a valid ZIP file.
        \item Click the "Analyze Dataset" button.
    \end{enumerate}
    \item \textbf{Expected result}: An alert with the message "Please enter an analysis name." is displayed.
    
\end{itemize}

\subsubsection{Calls uploadFiles and displays success message when form is submitted}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{uploadFiles} function is called and a success message is displayed when the form is submitted.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Upload a valid ZIP file.
        \item Enter an analysis name.
        \item Click the "Analyze Dataset" button.
    \end{enumerate}
    \item \textbf{Expected result}: The \texttt{uploadFiles} function is called with the file and analysis name, and a success message "Analysis started successfully. You'll receive an email with the results once it's done!" is displayed.
\end{itemize}

\subsubsection{Handles file drop event}
\begin{itemize}
    \item \textbf{Description}: Ensures the file drop event is handled correctly when a file is dropped on the component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Simulate a file drop event with a valid ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: The file is uploaded successfully.
\end{itemize}

\subsection{UploadResults Component}

\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{UploadResults} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Check for the presence of specific text.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the text "View Results" and "Upload your results ZIP file to view the analysis."
\end{itemize}

\subsubsection{Displays an error message when a non-ZIP file is uploaded}
\begin{itemize}
    \item \textbf{Description}: Ensures an error message is displayed when a non-ZIP file is uploaded.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Upload a non-ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: An error message "Only ZIP files are allowed." is displayed.
\end{itemize}

\subsubsection{Displays an error message when no JSON file is found in the ZIP}
\begin{itemize}
    \item \textbf{Description}: Ensures an error message is displayed when no JSON file is found in the ZIP file.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Upload a ZIP file without a JSON file.
    \end{enumerate}
    \item \textbf{Expected result}: An error message "No JSON file found in the ZIP." is displayed.
\end{itemize}

\subsubsection{Displays a success message when a valid ZIP file is uploaded}
\begin{itemize}
    \item \textbf{Description}: Ensures a success message is displayed when a valid ZIP file is uploaded.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Upload a valid ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: A success message "Results file uploaded successfully." is displayed.
\end{itemize}

\subsubsection{Displays a warning message when trying to view results without uploading a file}
\begin{itemize}
    \item \textbf{Description}: Ensures a warning message is displayed when trying to view results without uploading a file.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Click the "View Results" button.
    \end{enumerate}
    \item \textbf{Expected result}: A warning message "Please upload a results file first." is displayed.
\end{itemize}

\subsubsection{Handles errors during ZIP file processing}
\begin{itemize}
    \item \textbf{Description}: Ensures errors during ZIP file processing are handled correctly and an error message is displayed to the user.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Upload a ZIP file that cannot be processed.
    \end{enumerate}
    \item \textbf{Expected result}: An error message "Invalid ZIP file or corrupt JSON." is logged into the console.
\end{itemize}

\subsubsection{Handles file drop event}
\begin{itemize}
    \item \textbf{Description}: Ensures the file drop event is handled correctly when a file is dropped on the component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Simulate a file drop event with a valid ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: The file is uploaded successfully.
\end{itemize}

\subsection{Index (Landing Page) Component}

\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{Index} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
        \item Check for the presence of specific text.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the text "Verify Academic and Contest Submissions" and "Empower educators and contest administrators with advanced AI-powered plagiarism detection. Ensure the integrity of student work and competition submissions."
\end{itemize}

\subsubsection{Displays loading spinner when loading}
\begin{itemize}
    \item \textbf{Description}: Ensures the loading spinner is displayed when the component is in the loading state.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: \texttt{isLoading} is true
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isLoading} as true.
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The text "Preparing your experience..." is displayed.
\end{itemize}

\subsubsection{Calls loginWithRedirect when Get Started button is clicked}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{loginWithRedirect} function is called when the "Get Started" button is clicked.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
        \item Click the "Get Started" button.
    \end{enumerate}
    \item \textbf{Expected result}: The \texttt{loginWithRedirect} function is called.
\end{itemize}

\subsubsection{Navigates to /home when authenticated}
\begin{itemize}
    \item \textbf{Description}: Ensures the user is redirected to the \texttt{Home} page when authenticated.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isAuthenticated} as true.
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The user is redirected to the \texttt{Home} page.
\end{itemize}

\subsubsection{Displays features section}
\begin{itemize}
    \item \textbf{Description}: Ensures the features section is displayed on the \texttt{Index} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The features section is displayed with the text "Why Choose SyntaxSentinals?" along with the features.
\end{itemize}

\subsubsection{Displays how it works section}
\begin{itemize}
    \item \textbf{Description}: Ensures the "How It Works" section is displayed on the \texttt{Index} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The "How It Works" section is displayed along with the steps.
\end{itemize}

\subsubsection{Displays footer}
\begin{itemize}
    \item \textbf{Description}: Ensures the footer is displayed on the \texttt{Index} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The footer is displayed with the text "SyntaxSentinals" and the links.

\end{itemize}

\subsection{Upload File Box Component}
\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{UploadFileBox} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadFileBox} component.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the text "Upload a ZIP file" and the file input.
\end{itemize}

\subsubsection{Uploads a valid file}
\begin{itemize}
    \item \textbf{Description}: Ensures a valid file is uploaded using the \texttt{UploadFileBox} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadFileBox} component.
        \item Upload a valid ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: The file is uploaded successfully.
\end{itemize}

\subsubsection{Removes a file}
\begin{itemize}
    \item \textbf{Description}: Ensures a file is removed using the \texttt{UploadFileBox} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: A file is uploaded
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadFileBox} component.
        \item Click the "Remove" button.
    \end{enumerate}
    \item \textbf{Expected result}: The file is removed.
\end{itemize}

\subsubsection{Clears the file list when clearFiles is true}
\begin{itemize}
    \item \textbf{Description}: Ensures the file list is cleared when the \texttt{clearFiles} prop is true.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: A file is uploaded
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadFileBox} component with the \texttt{clearFiles} prop set to true.
    \end{enumerate}
    \item \textbf{Expected result}: The file list is cleared.
\end{itemize}

\subsection{NavBar Component}
\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{NavBar} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{NavBar} component.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the logo and navigation links.
    
\end{itemize}

\subsubsection{Displays the login button when not authenticated}
\begin{itemize}
    \item \textbf{Description}: Ensures the login button is displayed when the user is not authenticated.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isAuthenticated} as false.
        \item Render the \texttt{NavBar} component.
    \end{enumerate}
    \item \textbf{Expected result}: The login button is displayed.
\end{itemize}

\subsubsection{Displays the logout button when authenticated}
\begin{itemize}
    \item \textbf{Description}: Ensures the logout button is displayed when the user is authenticated.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isAuthenticated} as true.
        \item Render the \texttt{NavBar} component.
    \end{enumerate}
    \item \textbf{Expected result}: The logout button is displayed.
\end{itemize}

\subsubsection{Calls loginWithRedirect when Log In button is clicked}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{loginWithRedirect} function is called when the "Log In" button is clicked.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \item \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isAuthenticated} as false.
        \item Render the \texttt{NavBar} component.
        \item Click the "Log In" button.
    \end{enumerate}
    \item \textbf{Expected result}: The \texttt{loginWithRedirect} function is called.
\end{itemize}

\section{Changes Due to Testing}

\wss{This section should highlight how feedback from the users and from 
the supervisor (when one exists) shaped the final product.  In particular 
the feedback from the Rev 0 demo to the supervisor (or to potential users) 
should be highlighted.}

\section{Automated Testing}
The tests mentioned in the \texttt{Unit Testing} section are automated using the Jest testing framework. The tests are run using the command \texttt{npm test}.

To ensure continuous integration and automated testing, we have set up a GitHub Actions workflow. The workflow is triggered on every push and pull request to the repository. It runs the tests on an Ubuntu environment with Node.js version 18.x.

\subsection{Testing Workflow Configuration}

The following is the configuration of the GitHub Actions workflow used for automated testing:

\begin{verbatim}
name: Run Frontend Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-20.04

    strategy:
      matrix:
        node-version: [18.x]

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}

    - name: Install dependencies
      run: npm install
      working-directory: ./src/frontend

    - name: Run tests
      run: npm test
      working-directory: ./src/frontend
\end{verbatim}

\subsection{Testing Workflow Steps}

\begin{itemize}
    \item \textbf{Checkout repository}: This step uses the \texttt{actions/checkout@v4} action to check out the repository code.
    \item \textbf{Set up Node.js}: This step uses the \texttt{actions/setup-node@v4} action to set up Node.js with the specified version (18.x).
    \item \textbf{Install dependencies}: This step runs \texttt{npm install} to install the necessary dependencies for the frontend.
    \item \textbf{Run tests}: This step runs \texttt{npm test} to execute the Jest tests for the frontend.
\end{itemize}

By using this automated testing workflow, we ensure that our code is continuously tested and validated, providing immediate feedback on any issues that may arise from changes in the codebase.

\subsection{Test Coverage Workflow Configuration}

In addition to running unit tests, we have configured a separate GitHub Actions workflow to generate a test coverage report using Jest. This helps us monitor the extent to which our code is covered by automated tests.

The following is the configuration for the test coverage workflow:

\begin{verbatim}
name: Run Frontend Unit Tests

on: [push, pull_request]

jobs:
coverage:
runs-on: ubuntu-20.04

steps:
  - name: Checkout repository
    uses: actions/checkout@v3

  - name: Run Jest Coverage Report
    uses: ArtiomTr/jest-coverage-report-action@v2
    with:
      working-directory: ./src/frontend
      github-token: ${{ secrets.GITHUB_TOKEN }}

\end{verbatim}

\subsection{Test Coverage Workflow Steps}

\begin{itemize}
\item \textbf{Checkout repository}: This step uses the \texttt{actions/checkout@v3} action to check out the repository code.
\item \textbf{Run Jest Coverage Report}: This step uses the \\ \texttt{ArtiomTr/jest-coverage-report-action@v2} action to generate a coverage report for the Jest tests in the frontend directory.
\end{itemize}

By incorporating test coverage reporting into our workflow, we can ensure that our automated tests provide sufficient coverage of the codebase, identifying untested areas and improving software reliability.
		
\section{Trace to Requirements}
		
\section{Trace to Modules}		

\section{Code Coverage Metrics}

The below report is for the frontend codebase of SyntaxSentinals. The backend has not been tested with a testing framework yet.
Instead we ensure the backend is working as expected by manually testing the API endpoints using Postman and ensuring the server is sent the 
expected request via the frontend testing suite. \\

The test coverage report provides the following key metrics:

\begin{itemize}
\item \textbf{Statements}: 94.35\% (234/248)
\item \textbf{Branches}: 81.03\% (47/58)
\item \textbf{Functions}: 86.79\% (46/53)
\item \textbf{Lines}: 96.18\% (227/236)
\end{itemize}

\newpage

Below is a breakdown of coverage by individual files:

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Directory} & \textbf{Statements} & \textbf{Branches} & \textbf{Functions} & \textbf{Lines} \\
    \hline
    Features.tsx & 100\% & 100\% & 100\% & 100\% \\
    Navbar.tsx & 100\% & 100\% & 100\% & 100\% \\
    UploadBox.tsx & 96.42\% & 84.21\% & 93.33\% & 98.14\% \\
    GetStarted.tsx & 100\% & 100\% & 100\% & 100\% \\
    Home.tsx & 90.32\% & 69.56\% & 73.91\% & 93.16\% \\
    Button.tsx & 100\% & 66.66\% & 100\% & 100\% \\
    utils.ts & 100\% & 100\% & 100\% & 100\% \\
    AnalyzeFiles.tsx & 91.52\% & 75\% & 75\% & 94.54\% \\
    UploadResults.tsx & 89.23\% & 63.63\% & 72.72\% & 91.93\% \\
    \hline
    \end{tabular}
    \caption{Test Coverage Breakdown by Directory}
    \label{tab:coverage}
\end{table}

Some uncovered lines include:

\begin{itemize}
\item \textbf{UploadBox.tsx}: Line 26.
\item \textbf{button.tsx}: Line 44.
\item \textbf{AnalyzeFiles.tsx}: Lines 22, 25, 76.
\item \textbf{UploadResults.tsx}: Lines 20, 23, 50-51, 89.
\end{itemize}

These uncovered areas indicate potential gaps in test cases that should be addressed to achieve full coverage.

\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable?
  
  It was relatively straightforward to map out the tests we needed to focus on 
  based on the Verification and Validation (VnV) plan document. The VnV plan 
  provided clear expectations for the necessary tests, making it easier to 
  structure the deliverable around those objectives. We didn't need to perform 
  as much high-level reassessment of the overall design as we initially thought 
  we might and had to for previous deliverables. This made the process feel 
  more smooth, as we could concentrate on detailing how the tests would be 
  executed and aligning them with the deliverables outlined earlier in the 
  project. The clear documentation we had so far played a significant role in 
  creating a sense of continuity, which made writing this deliverable easier.

  \item What pain points did you experience during this deliverable, and how
    did you resolve them?

    One of the main challenges we encountered was coming up with compelling 
    data to verify the non-functional requirements for the project. While we 
    were able to conduct usability tests with peers who rated our UI on a scale 
    from 1 to 5, we felt unsure about whether this feedback would provide 
    enough substance to support the NFRs. We sought advice from our TA, who reassured 
    us that such usability tests, particularly when supplemented by qualitative 
    feedback and user comments, could be deemed a reasonable and valid dataset 
    for justifying the passing of NFR tests. This gave us the confidence to proceed 
    with the existing data and treat it as adequate for demonstrating that the UI 
    was user-friendly and met our usability NFRs.

    Another pain point revolved around selecting the right framework for frontend 
    testing. There was some ambiguity about which testing framework 
    would best suit the needs of our application, especially given the variety 
    of tools and options available for testing JavaScript-based web applications. 
    We resolved this by conducting thorough research into various options. This let 
    us determine that Jest would be the most appropriate choice for our project. 
    Jest's'ease of use, and the fact that it was already well integrated with React 
    (the framework we were using for the frontend) made it a great fit.

  \item Which parts of this document stemmed from speaking to your client(s) or
  a proxy (e.g. your peers)? Which ones were not, and why?

  The majority of this document stemmed directly from team discussions. 
  Since we were working closely together and had a good grasp of the 
  technical specifications, most of the deliverable's content was informed 
  by our team's perspectives and experience. The only sections which required 
  client/peer assessment was anything related to the usability tests. As developers, 
  it was difficult for us to maintain impartiality about the UI's intuitiveness and 
  ease of use. We needed unbiased feedback to assess how intuitive and usable our 
  interface truly was, so we conducted usability tests with external users 
  (mainly classmates) to gather their insights. Their feedback on how easily they 
  could interact with the interface was necessary to assign a pass or fail for some tests. 
  Thus, while most of the document was internally driven, the usability testing portion 
  was based on direct feedback from external stakeholders to ensure that our assumptions 
  about the UI held up in real-world use.

  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.)

While the VnV plan provided a great starting point, we did encounter some 
deviations from the original plan as the project evolved. Aspects such as cloud 
deployment and metrics for the NLP model ended up being much more complex and 
time-consuming than we originally anticipated. We had not factored in the 
amount of work required for cloud integration and the creation of monitoring 
tools for the NLP model's performance, nor how much work it would be to get 
the project to a stage where it was ready for either of those components. These changes 
meant that certain tests had to be postponed to a later phase of the project, and we had to 
adjust the scope of the tests that could be conducted in the current phase.

Another area of change was related to user notification systems. Initially, the VnV 
plan assumed that notifications about long processing times (e.g., 10 minutes or more) 
would be shown directly in the UI. However, after some reflection and considering user 
behavior, we realized that it was unlikely that users would remain engaged with the UI 
during long processing times. Therefore, we decided to send these notifications via 
email only instead, which led to a change in the VnV testing scope. These modifications were 
a result of our growing understanding of user expectations. We found that such changes 
can be anticipated to some extent in future projects if we invest more time in the 
early stages to gather feedback, conduct prototyping, and set clear technical expectations.

While it was difficult to predict all of these changes upfront, the team was able to adjust 
quickly thanks to open communication and ascertaining of direction. If we had spent more 
time in earlier phases fully scoping out all potential technical complexities and user 
scenarios, we might have been able to avoid some of these surprises. That is a goal for 
future projects we participate in.
\end{enumerate}

\end{document}
