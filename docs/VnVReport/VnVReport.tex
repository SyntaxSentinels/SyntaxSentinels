\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can reference the SRS tables if needed}

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

This document ...

\section{Functional Requirements Evaluation}
\subsubsection{Plagiarism Analysis Input Upload Tests}

Applies to all FRs involving uploading code snippets to the system. This currently 
covers FR-1 in subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-1\\}

Control: Automatic.
					
Initial State: system is active and set up in spot to receive code snippet 
uploads, zero or more existing snippets are currently uploaded.
					
Input: code snippet file(s).
					
Output: System signifies input was received and continues activity without 
error, awaiting further action directives (such as uploading more files or 
initiating analysis).

Test Case Derivation: The system will look to continually receive code 
snippets from user as part of a necessary step to initiate plagiarism analysis.
					
How test will be performed: A script will open the system in the appropriate spot for 
receiving code and will upload files in varying amounts, one batch at a time, all the 
while inspecting for failure during the process. Script will be integrated into CI/CD
pipeline to ensure continued system functionality (types: dynamic, functional).

Expected result: The script should run and successfully upload the files, without 
encountering any errors due to the upload functionality

Actual Result: No errors from uplading

Evaluation: Pass
					

\end{enumerate}

\subsubsection{Plagiarism Analysis Result Tests}

Applies to all FRs involving immediate output of plagiarism analysis. This 
currently covers FR-2, FR-4, and FR-5 in subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-2\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for initiating analysis, 
2 or more code snippets have been uploaded.
					
Input: command to initiate analysis (button, enter, etc.).
					
Output: System presents list of similarity scores for all code snippet pairings 
along with corresponding threshold scores and any code snippet pairing that 
has exceeded its threshold has been flagged. System remains active, 
awaiting for action directives on what to do.

Test Case Derivation: The system should possess an algorithm that analyzes code 
snippets and produces similarity scores and threshold scores in return. These 
scores should be produced between every pairing of code snippets as the 
plagiarism is a relative assessment between one code piece and another. The 
system must able to pass this algorithm received code snippets and use its
results to flag any pairing that exceeded its threshold before returning from
its analysis state to the user for further interaction.

How test will be performed: A script will open the system's spot to initiate analysis 
with 2 or more code snippets already inserted into the system, and command for 
starting analysis will be entered. Errors will be inspected for 
up until similarity scores and thresholds have been presented. By this point,
code flaggings should also be available to ascertain. (type: dynamic, functional).

Expected result: The analysis should not throw any errors while the script runs,
up until the analysis provides a response

Actual result: No errors were observed during script execution

Evaluation: Pass

					

\end{enumerate}

\subsubsection{Guide Documentation Generation Tests}

Applies to all FRs involving guide documentation generation for user. This 
currently covers FR-3 in subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-3\\}

Control: Automatic.
					
Initial State: System is active and set up in spot for generating guide 
documentation.
					
Input: command for documentation presentation (button, enter, etc.).
					
Output: System generates documentation in a viewable medium, such as text or a 
PDF file, and remains active while awaiting for further action directives.

Test Case Derivation: system possesses guide documentation which it should be 
capable of transferring to user to provide guidance on use cases of system 
when prompted.

How test will be performed: A script will open the system's spot for generating 
guide documentation and command for guide generating documentation will be 
entered, all the while inspecting for error. It will check for missing 
documentation at the end. (type: dynamic, functional). 

Expected result: Generation of documentation successful

Actual result: TBD when feature is implemented

Evaluation: TBD

\end{enumerate}

\subsubsection{Report Documentation Generation Tests}

Applies to all FRs involving analysis report documentation generation for 
user. This currently covers FR-6 of subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-4\\}

Control: Automatic.
					
Initial State: System is active and set up in spot for report generation, 
most recent plagiarism analysis has been completed since after system activation 
and its results are available.
					
Input: command for report documentation generation (button, enter, etc.).
					
Output: System provides report documentation in a viewable medium, such
as text or a PDF file, and remains active awaiting for further action 
directives.

Test Case Derivation: The system should possess the ability to aggregate
the results of the plagiarism analysis which has most recently occurred 
and insert them into a template that can summarize all findings to a user,
which will become the report document given.

How test will be performed: A script will have the system conduct a 
plagiarism analysis to provide inputs for the report generation as this
simulates what must happen before a user can unlock report generation. 
If successful, it will proceed to open the spot for generating a report 
where the command for generating a report will be entered, all the while 
inspecting for errors during the process. It will check for missing 
documentation at the end (type: dynamic, functional).			

Expected result: Generation of report is successful, with no errors encountered

Actual result: Report was successfully generated, no errors were found

Evaluation: Pass

\end{enumerate}

\subsubsection{Account Creation Tests}

Applies to all FRs involving creating an account within the system. This currently
covers FR-7 of subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-5\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for account creation.
					
Input: command for account creation (button, enter, etc.) alongside account 
user email and password, and the email is not associated with any existing 
account.
					
Output: System indicates account creation was successfully created for 
logging in with (evidenced by existence of email within created account list)
and remains active, awaiting further action directives.

Test Case Derivation: The system should be able to assess a set of account 
credentials is not yet within the system and proceed to add this set to the 
set of accounts that can be logged in with.

How test will be performed: A script will open the system's spot for account creation, 
email and password not associated with any existing account will be given in the 
appropriate area, and command for account creation will be entered; all the while inspecting 
for any failure mode within the process. Created account will be inspected for at the end.
(type: dynamic, functional).

Expected result: An account should be successfully created, while the system avids any failure modes.

Actual result: Account successfully created, and no failure modes were exhibited

Evaluation: Pass

\item{test-FR-6\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for account creation.
					
Input: command for account creation (button, enter, etc.) alongside account 
user email and password, and the email is associated with an existing 
account.
					
Output: System notifies account was not possible to create for logging 
in with and remains active, awaiting further action directives.

Test Case Derivation: A set of pre-existing account credentials should not be 
possible to create an account with. Otherwise, account creation is arbitrary 
and not truly provided by the system as any set of account credentials are not 
tied to any particular account.

How test will be performed: A script will open the system's spot for account 
creation, email and password asssociated with an existing account will be 
given in the appropriate area, and command for account creation will be 
entered; all the while inspecting for any failure mode within the process 
(type: dynamic, functional).
		
Expected result: Account creation should fail and the system will await further input from the user

Actual result: Account creation fails, and the system does not proceed

Evaluation: Pass 

\end{enumerate}

\subsubsection{Account Login Tests}

Applies to all FRs involving logging into account within the system. This 
currently covers FR-8 of subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-7\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for account 
login.
					
Input: command for account login (button, enter, etc.) alongside account 
user email and password, and the email is associated with an existing 
account.
					
Output: System notifies account login was successful and and remains 
active, awaiting further action directives.

Test Case Derivation: The system should be able to validate a set of 
pre-existing account credentials to facilitate login.

How test will be performed: A script will open the system's spot for account 
login, email and  password asssociated with existing account will be given 
in the appropriate area, and command for account login will be passed; all 
the while inspecting for any failure mode within the process (type: dynamic, 
functional).

Expected result: Logging into system with correct existing credentials should
succeed, and no errors should arise from logging in

Actual result: Logging in with existing credentials succeeds with no errors

Evaluation: Pass

\item{test-FR-8\\}

Control: Automatic.
					
Initial State: system is active and set up in spot for account login.
					
Input: command for account login (button, enter, etc.) alongside account 
user email and password, and the email is not associated with any existing 
account.
					
Output: System notifies account login was not successful and remains 
active, awaiting further action directives.

Test Case Derivation: A set of account credentials not associated an existing 
account will fail to allow login as the system should determine there is no 
account to validate against.

How test will be performed: A script will open system's spot for account login, 
email and password not asssociated with existing account with will be given in 
the appropriate area, and command for account login will be passed; all the 
while inspecting for any failure mode within the process (type: dynamic, functional).

Expected result: Logging in with invalid credentials should fail

Actual result: Logging in with invalid credentials fails

Evaluation: Pass
					
\end{enumerate}

\subsubsection{Result Email Tests}

Applies to all FRs involving emailing results of plagiarism analysis to users. 
This currently covers FR-9 of subsection 9.1 of the SRS \citep{SRS}.

\begin{enumerate}

\item{test-FR-9\\}

Control: Automatic.
					
Initial State: System is active and set up in spot for emailing results,
most recent plagiarism analysis has been completed since after system activation 
and its results are available.
					
Input: command for emailing result (button, enter, etc.) and email to 
receive results.
					
Output: System notifies email has been sent and remains active, awaiting 
further action directives. External to system, the specified email shall
contain a .zip file possessing results of the recent analysis.

Test Case Derivation: The system should possess the ability to condense results
into a .zip file upon demand, and it should proceed to carry out emailing this 
file after creating it.

How test will be performed: A script will have the system conduct a 
plagiarism analysis to provide inputs for emailing as this
simulates what must happen for a user before they are able to initiate
the email function. If successful, the script will continue to the system's spot 
for emailing results and the command for sending email will be entered alongside 
an email for receiving the results; all the while inspecting for errors in the 
process. The reception of the .zip file will be assessed at the end once 
the system is awaiting action directives (type: dynamic, functional).

Expected result: The system will send an e-mail to address associated with the
logged in account, containing a .zip file with plagiarism results
corresponding to the files uploaded

Actual result: Correct and corresponding .zip file was sent to the expected e-mail

Evaluation: Pass

\end{enumerate}

\subsubsection{Result Visualization Tests}

Applies to all FRs involving visualizing plagiarism analysis results provided 
by user in a .zip file. This currently covers FR-10 of subsection 9.1 of the SRS 
\citep{SRS}.


\begin{enumerate}

\item{test-FR-10\\}

Control: Automatic
					
Initial State: System is active and set up in the spot for inserting .zip 
files containing results to produce visualization.
					
Input: command for visualization (button, enter, etc.) and .zip file containing 
results.
					
Output: System provides visualization in viewable medium, such as image or file,
that corresponds to the results in the given .zip file.

Test Case Derivation: System should possess ability to parse .zip file for 
relevant contents and pass it into an internal visualization template that 
can be filled out with what was found before proceeding to transfer it to
the user.

How test will be performed: A script will open the system's spot for 
visualization creation and insert a .zip file containing plagiarism results 
before entering the command for creating a visualization. It will inspect for 
any errors during this process. At the end, the script will inspect for 
missing parts of the visualization.
\end{enumerate}
				
Expected result: A visualization is successfully created after uplading a .zip
file, with no errors observed during the process.

Actual result: A visualization was successfully created with no errors

Evaluation: Pass



\section{Nonfunctional Requirements Evaluation}

\subsection{Usability}
		
\subsection{Performance}

\subsection{etc.}
	
\section{Comparison to Existing Implementation}	

\subsection{Overview of Existing Solutions}

\subsubsection{MOSS (Measure of Software Similarity)}
MOSS is currently the standard tool for detecting code plagiarism in academic settings. Developed at Stanford University, it works by:
\begin{itemize}
    \item Tokenizing code into a sequence of symbols
    \item Using document fingerprinting to detect similar code segments
    \item Generating a similarity score based on matching sequences
    \item Providing a web interface to view overlapping code segments
\end{itemize}

\subsubsection{Other Notable Solutions}
\begin{itemize}
    \item \textbf{JPlag}: Uses a token-based approach similar to MOSS, but with different tokenization strategies
    \item \textbf{PLAGGIE}: Java-specific plagiarism detection tool used in academic environments
    \item \textbf{CodeMatch}: Commercial solution that uses both tokenization and metric-based methods
\end{itemize}

\subsection{Our Implementation vs. Existing Solutions}

\subsubsection{Current Approach}
Our implementation leverages CodeBERT, a pre-trained model for programming language understanding, to perform code pair comparison. Key aspects include:
\begin{itemize}
    \item Utilization of transformer-based neural networks to understand code semantics
    \item Direct comparison of code pairs to determine similarity
    \item A modern, user-friendly interface for result visualization and analysis
\end{itemize}

\subsubsection{Functional Comparison}

\begin{table}[h]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Feature} & \textbf{MOSS} & \textbf{Our CodeBERT Implementation} \\
\hline
Detection Method & Token-based fingerprinting & Neural representation and semantic understanding \\
\hline
Language Support & Multiple languages & Multiple languages (supported by CodeBERT) \\
\hline
Semantic Understanding & Limited (syntax-focused) & Enhanced (captures semantic meaning) \\
\hline
Resistance to Obfuscation & Low-moderate & Potentially higher \\
\hline
Speed & Fast for large submissions & Currently slower for large-scale comparisons \\
\hline
Visualization & Basic web interface & Modern, interactive UI \\
\hline
\end{tabular}
\caption{Comparison between MOSS and our CodeBERT implementation}
\label{tab:comparison}
\end{table}

\subsection{Advantages of Our Approach}

\begin{enumerate}
    \item \textbf{Semantic Understanding}: Unlike MOSS's purely syntactic approach, CodeBERT can potentially understand the meaning behind code, making it more difficult to fool with non-functional additions.

    \item \textbf{Context Awareness}: Our implementation considers the context in which code appears, potentially reducing false positives from common solutions to standard problems.

    \item \textbf{Modern Interface}: Our user interface provides a more intuitive experience for instructors reviewing potential plagiarism cases.
\end{enumerate}

\subsection{Current Limitations and Trade-offs}

\begin{enumerate}
    \item \textbf{Computational Requirements}: Neural models like CodeBERT require more computational resources than traditional approaches like MOSS.

    \item \textbf{Development Maturity}: MOSS has been refined over decades, while our solution is still in the early stages of development.

    \item \textbf{Validation}: Our approach needs more extensive testing across different programming languages and assignments to prove its effectiveness.

    \item \textbf{Explainability}: Neural network decisions can be less transparent than token-matching approaches, making it potentially harder to explain why certain code pairs were flagged.
\end{enumerate}

\subsection{Design Decision Justification}

\begin{enumerate}
    \item \textbf{Choice of CodeBERT}: We selected CodeBERT over other models because it was specifically pre-trained on code from multiple programming languages, making it well-suited for understanding code semantics across different languages used in academic settings.

    \item \textbf{Focus on Pair Comparison}: While MOSS compares each submission against all others, our current approach focuses on direct pair comparison, allowing for more detailed analysis of potentially plagiarized code.

    \item \textbf{UI Priority}: We invested in a high-quality UI early in development to facilitate easier testing and validation of our detection results.
\end{enumerate}

\subsection{Future Improvements}

\begin{enumerate}
    \item \textbf{Hybrid Approach}: Combining neural understanding with traditional token-based methods to leverage strengths of both approaches.

    \item \textbf{Performance Optimization}: Improving the computational efficiency to handle large class submissions more effectively.

    \item \textbf{Tunable Sensitivity}: Implementing adjustable thresholds for different assignment types and programming languages.
\end{enumerate}

\section{Unit Testing}
\subsection{AnalyzeFiles Component}

\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{AnalyzeFiles} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Check for the presence of specific text.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the text "Analyze a Dataset" and "Upload a dataset to analyze for potential code plagiarism. Use a ZIP file containing your project files."
\end{itemize}

\subsubsection{Displays an alert when no file is uploaded}
\begin{itemize}
    \item \textbf{Description}: Ensures an alert is displayed when no file is uploaded.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Click the "Analyze Dataset" button.
    \end{enumerate}
    \item \textbf{Expected result}: An alert with the message "Please upload a dataset file first." is displayed.
\end{itemize}

\subsubsection{Displays an alert when no analysis name is entered}
\begin{itemize}
    \item \textbf{Description}: Ensures an alert is displayed when no analysis name is entered.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Upload a valid ZIP file.
        \item Click the "Analyze Dataset" button.
    \end{enumerate}
    \item \textbf{Expected result}: An alert with the message "Please enter an analysis name." is displayed.
    
\end{itemize}

\subsubsection{Calls uploadFiles and displays success message when form is submitted}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{uploadFiles} function is called and a success message is displayed when the form is submitted.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Upload a valid ZIP file.
        \item Enter an analysis name.
        \item Click the "Analyze Dataset" button.
    \end{enumerate}
    \item \textbf{Expected result}: The \texttt{uploadFiles} function is called with the file and analysis name, and a success message "Analysis started successfully. You'll receive an email with the results once it's done!" is displayed.
\end{itemize}

\subsubsection{Handles file drop event}
\begin{itemize}
    \item \textbf{Description}: Ensures the file drop event is handled correctly when a file is dropped on the component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{AnalyzeFiles} component.
        \item Simulate a file drop event with a valid ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: The file is uploaded successfully.
\end{itemize}

\subsection{UploadResults Component}

\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{UploadResults} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Check for the presence of specific text.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the text "View Results" and "Upload your results ZIP file to view the analysis."
\end{itemize}

\subsubsection{Displays an error message when a non-ZIP file is uploaded}
\begin{itemize}
    \item \textbf{Description}: Ensures an error message is displayed when a non-ZIP file is uploaded.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Upload a non-ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: An error message "Only ZIP files are allowed." is displayed.
\end{itemize}

\subsubsection{Displays an error message when no JSON file is found in the ZIP}
\begin{itemize}
    \item \textbf{Description}: Ensures an error message is displayed when no JSON file is found in the ZIP file.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Upload a ZIP file without a JSON file.
    \end{enumerate}
    \item \textbf{Expected result}: An error message "No JSON file found in the ZIP." is displayed.
\end{itemize}

\subsubsection{Displays a success message when a valid ZIP file is uploaded}
\begin{itemize}
    \item \textbf{Description}: Ensures a success message is displayed when a valid ZIP file is uploaded.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Upload a valid ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: A success message "Results file uploaded successfully." is displayed.
\end{itemize}

\subsubsection{Displays a warning message when trying to view results without uploading a file}
\begin{itemize}
    \item \textbf{Description}: Ensures a warning message is displayed when trying to view results without uploading a file.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Click the "View Results" button.
    \end{enumerate}
    \item \textbf{Expected result}: A warning message "Please upload a results file first." is displayed.
\end{itemize}

\subsubsection{Handles errors during ZIP file processing}
\begin{itemize}
    \item \textbf{Description}: Ensures errors during ZIP file processing are handled correctly and an error message is displayed to the user.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Upload a ZIP file that cannot be processed.
    \end{enumerate}
    \item \textbf{Expected result}: An error message "Invalid ZIP file or corrupt JSON." is logged into the console.
\end{itemize}

\subsubsection{Handles file drop event}
\begin{itemize}
    \item \textbf{Description}: Ensures the file drop event is handled correctly when a file is dropped on the component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadResults} component.
        \item Simulate a file drop event with a valid ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: The file is uploaded successfully.
\end{itemize}

\subsection{Index (Landing Page) Component}

\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{Index} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
        \item Check for the presence of specific text.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the text "Verify Academic and Contest Submissions" and "Empower educators and contest administrators with advanced AI-powered plagiarism detection. Ensure the integrity of student work and competition submissions."
\end{itemize}

\subsubsection{Displays loading spinner when loading}
\begin{itemize}
    \item \textbf{Description}: Ensures the loading spinner is displayed when the component is in the loading state.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: \texttt{isLoading} is true
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isLoading} as true.
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The text "Preparing your experience..." is displayed.
\end{itemize}

\subsubsection{Calls loginWithRedirect when Get Started button is clicked}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{loginWithRedirect} function is called when the "Get Started" button is clicked.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
        \item Click the "Get Started" button.
    \end{enumerate}
    \item \textbf{Expected result}: The \texttt{loginWithRedirect} function is called.
\end{itemize}

\subsubsection{Navigates to /home when authenticated}
\begin{itemize}
    \item \textbf{Description}: Ensures the user is redirected to the \texttt{Home} page when authenticated.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isAuthenticated} as true.
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The user is redirected to the \texttt{Home} page.
\end{itemize}

\subsubsection{Displays features section}
\begin{itemize}
    \item \textbf{Description}: Ensures the features section is displayed on the \texttt{Index} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The features section is displayed with the text "Why Choose SyntaxSentinals?" along with the features.
\end{itemize}

\subsubsection{Displays how it works section}
\begin{itemize}
    \item \textbf{Description}: Ensures the "How It Works" section is displayed on the \texttt{Index} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The "How It Works" section is displayed along with the steps.
\end{itemize}

\subsubsection{Displays footer}
\begin{itemize}
    \item \textbf{Description}: Ensures the footer is displayed on the \texttt{Index} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{Index} component.
    \end{enumerate}
    \item \textbf{Expected result}: The footer is displayed with the text "SyntaxSentinals" and the links.

\end{itemize}

\subsection{Upload File Box Component}
\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{UploadFileBox} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadFileBox} component.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the text "Upload a ZIP file" and the file input.
\end{itemize}

\subsubsection{Uploads a valid file}
\begin{itemize}
    \item \textbf{Description}: Ensures a valid file is uploaded using the \texttt{UploadFileBox} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadFileBox} component.
        \item Upload a valid ZIP file.
    \end{enumerate}
    \item \textbf{Expected result}: The file is uploaded successfully.
\end{itemize}

\subsubsection{Removes a file}
\begin{itemize}
    \item \textbf{Description}: Ensures a file is removed using the \texttt{UploadFileBox} component.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: A file is uploaded
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadFileBox} component.
        \item Click the "Remove" button.
    \end{enumerate}
    \item \textbf{Expected result}: The file is removed.
\end{itemize}

\subsubsection{Clears the file list when clearFiles is true}
\begin{itemize}
    \item \textbf{Description}: Ensures the file list is cleared when the \texttt{clearFiles} prop is true.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: A file is uploaded
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{UploadFileBox} component with the \texttt{clearFiles} prop set to true.
    \end{enumerate}
    \item \textbf{Expected result}: The file list is cleared.
\end{itemize}

\subsection{NavBar Component}
\subsubsection{Renders the component}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{NavBar} component renders correctly.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Render the \texttt{NavBar} component.
    \end{enumerate}
    \item \textbf{Expected result}: The component renders with the logo and navigation links.
    
\end{itemize}

\subsubsection{Displays the login button when not authenticated}
\begin{itemize}
    \item \textbf{Description}: Ensures the login button is displayed when the user is not authenticated.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isAuthenticated} as false.
        \item Render the \texttt{NavBar} component.
    \end{enumerate}
    \item \textbf{Expected result}: The login button is displayed.
\end{itemize}

\subsubsection{Displays the logout button when authenticated}
\begin{itemize}
    \item \textbf{Description}: Ensures the logout button is displayed when the user is authenticated.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isAuthenticated} as true.
        \item Render the \texttt{NavBar} component.
    \end{enumerate}
    \item \textbf{Expected result}: The logout button is displayed.
\end{itemize}

\subsubsection{Calls loginWithRedirect when Log In button is clicked}
\begin{itemize}
    \item \textbf{Description}: Ensures the \texttt{loginWithRedirect} function is called when the "Log In" button is clicked.
    \item \textbf{Type}: Automatic, Functional
    \item \textbf{Initial state}: None
    \item \textbf{Test case steps}:
    \item \begin{enumerate}
        \item Mock the \texttt{useAuth0} hook to return \texttt{isAuthenticated} as false.
        \item Render the \texttt{NavBar} component.
        \item Click the "Log In" button.
    \end{enumerate}
    \item \textbf{Expected result}: The \texttt{loginWithRedirect} function is called.
\end{itemize}

\section{Changes Due to Testing}

\wss{This section should highlight how feedback from the users and from 
the supervisor (when one exists) shaped the final product.  In particular 
the feedback from the Rev 0 demo to the supervisor (or to potential users) 
should be highlighted.}

\section{Automated Testing}
The tests mentioned in the \texttt{Unit Testing} section are automated using the Jest testing framework. The tests are run using the command \texttt{npm test}.

To ensure continuous integration and automated testing, we have set up a GitHub Actions workflow. The workflow is triggered on every push and pull request to the repository. It runs the tests on an Ubuntu environment with Node.js version 18.x.

\subsection{Testing Workflow Configuration}

The following is the configuration of the GitHub Actions workflow used for automated testing:

\begin{verbatim}
name: Run Frontend Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-20.04

    strategy:
      matrix:
        node-version: [18.x]

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}

    - name: Install dependencies
      run: npm install
      working-directory: ./src/frontend

    - name: Run tests
      run: npm test
      working-directory: ./src/frontend
\end{verbatim}

\subsection{Testing Workflow Steps}

\begin{itemize}
    \item \textbf{Checkout repository}: This step uses the \texttt{actions/checkout@v4} action to check out the repository code.
    \item \textbf{Set up Node.js}: This step uses the \texttt{actions/setup-node@v4} action to set up Node.js with the specified version (18.x).
    \item \textbf{Install dependencies}: This step runs \texttt{npm install} to install the necessary dependencies for the frontend.
    \item \textbf{Run tests}: This step runs \texttt{npm test} to execute the Jest tests for the frontend.
\end{itemize}

By using this automated testing workflow, we ensure that our code is continuously tested and validated, providing immediate feedback on any issues that may arise from changes in the codebase.

\subsection{Test Coverage Workflow Configuration}

In addition to running unit tests, we have configured a separate GitHub Actions workflow to generate a test coverage report using Jest. This helps us monitor the extent to which our code is covered by automated tests.

The following is the configuration for the test coverage workflow:

\begin{verbatim}
name: Run Frontend Unit Tests

on: [push, pull_request]

jobs:
coverage:
runs-on: ubuntu-20.04

steps:
  - name: Checkout repository
    uses: actions/checkout@v3

  - name: Run Jest Coverage Report
    uses: ArtiomTr/jest-coverage-report-action@v2
    with:
      working-directory: ./src/frontend
      github-token: ${{ secrets.GITHUB_TOKEN }}

\end{verbatim}

\subsection{Test Coverage Workflow Steps}

\begin{itemize}
\item \textbf{Checkout repository}: This step uses the \texttt{actions/checkout@v3} action to check out the repository code.
\item \textbf{Run Jest Coverage Report}: This step uses the \\ \texttt{ArtiomTr/jest-coverage-report-action@v2} action to generate a coverage report for the Jest tests in the frontend directory.
\end{itemize}

By incorporating test coverage reporting into our workflow, we can ensure that our automated tests provide sufficient coverage of the codebase, identifying untested areas and improving software reliability.
		
\section{Trace to Requirements}
		
\section{Trace to Modules}		

\section{Code Coverage Metrics}

The below report is for the frontend codebase of SyntaxSentinals. The backend has not been tested with a testing framework yet.
Instead we ensure the backend is working as expected by manually testing the API endpoints using Postman and ensuring the server is sent the 
expected request via the frontend testing suite. \\

The test coverage report provides the following key metrics:

\begin{itemize}
\item \textbf{Statements}: 94.35\% (234/248)
\item \textbf{Branches}: 81.03\% (47/58)
\item \textbf{Functions}: 86.79\% (46/53)
\item \textbf{Lines}: 96.18\% (227/236)
\end{itemize}

\newpage

Below is a breakdown of coverage by individual files:

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Directory} & \textbf{Statements} & \textbf{Branches} & \textbf{Functions} & \textbf{Lines} \\
    \hline
    Features.tsx & 100\% & 100\% & 100\% & 100\% \\
    Navbar.tsx & 100\% & 100\% & 100\% & 100\% \\
    UploadBox.tsx & 96.42\% & 84.21\% & 93.33\% & 98.14\% \\
    GetStarted.tsx & 100\% & 100\% & 100\% & 100\% \\
    Home.tsx & 90.32\% & 69.56\% & 73.91\% & 93.16\% \\
    Button.tsx & 100\% & 66.66\% & 100\% & 100\% \\
    utils.ts & 100\% & 100\% & 100\% & 100\% \\
    AnalyzeFiles.tsx & 91.52\% & 75\% & 75\% & 94.54\% \\
    UploadResults.tsx & 89.23\% & 63.63\% & 72.72\% & 91.93\% \\
    \hline
    \end{tabular}
    \caption{Test Coverage Breakdown by Directory}
    \label{tab:coverage}
\end{table}

Some uncovered lines include:

\begin{itemize}
\item \textbf{UploadBox.tsx}: Line 26.
\item \textbf{button.tsx}: Line 44.
\item \textbf{AnalyzeFiles.tsx}: Lines 22, 25, 76.
\item \textbf{UploadResults.tsx}: Lines 20, 23, 50-51, 89.
\end{itemize}

These uncovered areas indicate potential gaps in test cases that should be addressed to achieve full coverage.

\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item Which parts of this document stemmed from speaking to your client(s) or
  a proxy (e.g. your peers)? Which ones were not, and why?
  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.)
\end{enumerate}

\end{document}
